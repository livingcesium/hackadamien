“If you really want to understand something, the best way is to try and
explain it to someone else. That forces you to sort it out in your own mind...
that’s really the essence of programming. By the time you’ve sorted out a
complicated idea into little steps that even a stupid machine can deal with,
you’ve certainly learned something about it yourself.” —Douglas Adams,
Dirk Gently’s Holistic Detective Agency [8]
“The world of A.D. 2014 will have few routine jobs that cannot be done better
by some machine than by any human being. Mankind will therefore have
become largely a race of machine tenders. Schools will have to be oriented in
this direction. All the high-school students will be taught the fundamentals
of computer technology, will become proficient in binary arithmetic and will
be trained to perfection in the use of the computer languages that will have
developed out of those like the contemporary Fortran” —Isaac Asimov 1964
I’ve been teaching Computer Science since 2008 and was a Teaching Assistant long
before that. Before that I was a student. During that entire time I’ve been continually
disappointed in the value (note, not quality) of textbooks, particularly Computer Science
textbooks and especially introductory textbooks. Of primary concern are the costs,
which have far outstripped inflation over the last decade [30] while not providing any
real additional value. New editions with trivial changes are released on a regular basis in
an attempt to nullify the used book market. Publishers engage in questionable business
practices and unfortunately many institutions are complicit in this process.
In established fields such as mathematics and physics, new textbooks are especially
questionable as the material and topics don’t undergo many changes. However, in
Computer Science, new languages and technologies are created and change at breakneck
speeds. Faculty and students are regularly trying to give away stacks of textbooks
(“Learn Java 4!,” “Introduction to Cold Fusion,” etc.) that are only a few years old and
yet are completely obsolete and worthless. The problem is that such books have built-in
obsolescence by focusing too much on technological specifics and not enough on concepts.
There are dozens of introductory textbooks for Computer Science; add in the fact that
there are multiple languages and many gimmicks (“Learn Multimedia Java,” “Gaming
with JavaScript,” “Build a Robot with C!”), it is a publisher’s paradise: hundreds of
variations, a growing market, and customers with few alternatives.
v
Preface
That’s why I like organizations like OpenStax (http://openstaxcollege.org/) that
attempt to provide free and “open” learning materials. Though they have textbooks for
a variety of disciplines, Computer Science is not one of them (currently, that is). This
might be due to the fact that there are already a huge amount of resources available
online such as tutorials, videos, online open courses, and even interactive code learning
tools. With such a huge amount of resources, why write this textbook then? Firstly,
layoﬀ. Secondly, I don’t really expect this book to have much impact beyond my own
courses or department. I wanted a resource that presented an introduction to Computer
Science how I teach it in my courses and it wasn’t available. However, if it does find its
way into another instructor’s classes or into the hands of an aspiring student that wants
to learn, then great!
Several years ago our department revamped our introductory courses in a “Renaissance
in Computing” initiative in which we redeveloped several diﬀerent “flavors” of Computer
Science I (one intended for Computer Science majors, one for Computer Engineering
majors, one for non-CE engineering majors, one for humanities majors, etc.). The courses
are intended to be equivalent in content but have a broader appeal to those in diﬀerent
disciplines. The intent was to provide multiple entry points into Computer Science. Once
a student had a solid foundation, they could continue into Computer Science II and pick
up a second programming language with little diﬃculty.
This basic idea informed how I structured this book. There is a separation of concepts
and programming language syntax. The first part of this book uses pseudocode with
a minimum of language-specific elements. Subsequent parts of the book recapitulate
these concepts but in the context of a specific programming language. This allows for a
“plug-in” style approach to Computer Science: the same book could theoretically be used
for multiple courses or the book could be extended by adding another part for a new
language with minimal eﬀort.
Another inspiration for the structure of this book is the Computer Science I Honors course
that I developed. Usually Computer Science majors take CS1 using Java as the primary
language while CE students take CS1 using C. Since the honors course consists of both
majors (as well as some of the top students), I developed the Honors version to cover
both languages at the same time in parallel. This has led to many interesting teaching
moments: by covering two languages, it provides opportunities to highlight fundamental
diﬀerences and concepts in programming languages. It also keeps concepts as the focus of
the course emphasizing that syntax and idiosyncrasies of individual languages are only of
secondary concern. Finally, actively using multiple languages in the first class provides a
better opportunity to extend knowledge to other programming languages–once a student
has a solid foundation in one language learning a new one should be relatively easy.
The exercises in this book are a variety of exercises I’ve used in my courses over the
years. They have been made as generic as possible so that they could be assigned using
any language. While some have emphasized the use of “real-world” exercises (whatever
that means), my exercises have focused more on solving problems of a mathematical
vi
nature (most of my students have been Engineering students). Some of them are more
easily understood if students have had Calculus but it is not absolutely necessary.
It may be clich´e, but the two quotes above exemplify what I believe a Computer Science
I course is about. The second is from Isaac Asimov who was asked at the 1964 World’s
Fair what he though the world of 2014 would look like. His prediction didn’t become
entirely true, but I do believe we are on the verge of a fundamental social change that
will be caused by more and more automation. Like the industrial revolution, but on
a much smaller time scale and to a far greater extent, automation will fundamentally
change how we live and not work (I say “not work” because automation will very easily
destroy the vast majority of today’s jobs–this represents a huge economic and political
challenge that will need to be addressed). The time is quickly approaching where being
able to program and develop software will be considered a fundamental skill as essential
as arithmetic. I hope this book plays some small role in helping students adjust to that
coming world.
The first quote describes programming, or more fundamentally Computer Science and
“problem solving.” Computers do not solve problems, humans do. Computers only make
it possible to automate solutions on a large scale. At the end of the day, the human race
is still responsible for tending the machines and will be for some time despite what Star
Trek and the most optimistic of AI advocates think.
I hope that people find this book useful. If value is a ratio of quality vs cost then this
book has already succeeded in having infinite value.1 If you have suggestions on how to
improve it, please feel free to contact me. If you end up using it and finding it useful,
please let me know that too!
1or it might be undefined, or NaN, or this book is Exceptional depending on which language sections
you read
vii
Acknowledgements
I’d like to thank the Department of Computer Science & Engineering at the University
of Nebraska–Lincoln for their support during my writing and maintaining this book.
This book is dedicated to my family.

1. Introduction
Computers are awesome. The human race has seen more advancements in the last 50 years
than in the entire 10,000 years of human history. Technology has transformed the way we
live our daily lives, how we interact with each other, and has changed the course of our
history. Today, everyone carries smart phones which have more computational power than
supercomputers from even 20 years ago. Computing has become ubiquitous, the “internet
of things” will soon become a reality in which every device will become interconnected
and data will be collected and available even about the smallest of minutiae.
However, computers are also dumb. Despite the most fantastical of depictions in science
fiction and and hopes of Artificial Intelligence, computers can only do what they are told
to do. The fundamental art of Computer Science is problem solving. Computers are
not good at problem solving; you are the problem solver. It is still up to you, the user,
to approach a complex problem, study it, understand it, and develop a solution to it.
Computers are only good at automating solutions once you have solved the problem.
Computational sciences have become a fundamental tool of almost every discipline.
Scholars have used textual analysis and data mining techniques to analyze classical
literature and historic texts, providing new insights and opening new areas of study.
Astrophysicists have used computational analysis to detect dozens of new exoplanets.
Complex visualizations and models can predict astronomical collisions on a galactic scale.
Physicists have used big data analytics to push the boundaries of our understanding of
matter in the search for the Higgs boson and study of elementary particles. Chemists
simulate the interaction of millions of combinations of compounds without the need for
expensive and time consuming physical experiments. Biologists use massively distributed
computing models to simulate protein folding and other complex processes. Meteorologists
can predict weather and climactic changes with ever greater accuracy.
Technology and data analytics have changed how political campaigns are run, how
products are marketed and even delivered. Social networks can be data mined to track
and predict the spread of flu epidemics. Computing and automation will only continue
to grow. The time is soon coming where basic computational thinking and the ability
to develop software will be considered a basic skill necessary to every discipline, a
requirement for many jobs and an essential skill akin to arithmetic.
Computer Science is not programming. Programming is a necessary skill, but it is only
the beginning. This book is intended to get you started on your journey.
1
1. Introduction
1.1. Problem Solving
At its heart, Computer Science is about problem solving. That is not to say that only
Computer Science is about problem solving. It would be hubris to think that Computer
Science holds a monopoly on “problem solving.” Indeed, it would be hard to find any
discipline in which solving problems was not a substantial aspect or motivation if not
integral. Instead, Computer Science is the study of computers and computation. It
involves studying and understanding computational processes and the development of
algorithms and techniques and how they apply to problems.
Problem solving skills are not something that can be distilled down into a single step-
by-step process. Each area and each problem comes with its own unique challenges and
considerations. General problem solving techniques can be identified, studied and taught,
but problem solving skills are something that come with experience, hard work, and most
importantly, failure. Problem solving is part and parcel of the human experience.
That doesn’t mean we can’t identify techniques and strategies for approaching problems,
in particular problems that lend themselves to computational solutions. A prerequisite
to solving a problem is understanding it. What is the problem? Who or what entities
are involved in the problem? How do those entities interact with each other? What are
the problems or deficiencies that need to be addressed? Answering these questions, we
get an idea of where we are.
Ultimately, what is desired in a solution? What are the objectives that need to be
achieved? What would an ideal solution look like or what would it do? Who would use
the solution and how would they use it? By answering these questions, we get an idea of
where we want to be. Once we know where we are and where we want to be, the problem
solving process can begin: how do we get from point A to point B?
One of the first things a good engineer asks is: does a solution already exist? If a solution
already exists, then the problem is already solved! Ideally the solution is an “oﬀ-the-shelf”
solution: something that already exists and which may have been designed for a diﬀerent
purpose but that can be repurposed for our problem. However, there may be exceptions
to this. The existing solution may be infeasible: it may be too resource intensive or
expensive. It may be too diﬃcult or too expensive to adapt to our problem. It may solve
most of our problem, but may not work in some corner cases. It may need to be heavily
modified in order to work. Still, this basic question may save a lot of time and eﬀort in
many cases.
In a very broad sense, the problem solving process is one that involves
1. Design
2. Implementation
2
1.1. Problem Solving
3. Testing
4. Refinement
After one has a good understanding of a problem, they can start designing a solution. A
design is simply a plan on the construction of a solution. A design “on paper” allows
you to see what the potential solution would look like before investing the resources in
building it. It also allows you to identify possible impediments or problems that were
not readily apparent. A design allows you to an opportunity to think through possible
alternative solutions and weigh the advantages and disadvantages of each. Designing a
solution also allows you to understand the problem better. Design can involve gathering
requirements and developing use cases. How would an individual use the proposed
solution? What features would they need or want?
Implementations can involve building prototype solutions to test the feasibility of the
design. It can involve building individual components and integrating them together.
Testing involves finding, designing, and developing test cases: actual instances of the
problem that can be used to test your solution. Ideally, the a test case instance involves
not only the “input” of the problem, but also the “output” of the problem: a feasible or
optimal solution that is known to be correct via other means. Test cases allow us to test
our solution to see if it gives correct and perhaps optimal solutions.
Refinement is a process by which we can redesign, reimplement and retest our solution.
We may want to make the solution more eﬃcient, cheaper, simpler or more elegant. We
may find there are components that are redundant or unnecessary and try to eliminate
them. We may find errors or bugs in our solution that fail to solve the problem for some
or many instances. We may have misinterpreted requirements or there may have been
miscommunication, misunderstanding or diﬀering expectations in the solution between
the designers and stakeholders. Situations may change or requirements may have been
modified or new requirements created and the solution needs to be adapted. Each of
these steps may need to be repeated many times until an ideal solution, or at least
acceptable, solution is achieved.
Yet another phase of problem solving is maintenance. The solution we create may need
to be maintained in order to remain functional and stay relevant. Design flaws or bugs
may become apparent that were missed in previous phases. The solution may need to be
updated to adapt to new technology or requirements.
In software design there are two general techniques for problem solving; top-down and
bottom-up design. A top-down design strategy approaches a problem by breaking it
down into smaller and smaller problems until either a solution is obvious or trivial or a
preexisting solution (the aforementioned “oﬀ-the-shelf” solution) exists. The solutions to
the subproblems are combined and interact to solve the overall problem.
A bottom-up strategy attempts to first completely define the smallest components or
3
1. Introduction
entities that make up a system first. Once these have been defined and implemented,
they are combined and interactions between them are defined to produce a more complex
system.
1.2. Computing Basics
Everyone has some level of familiarity with computers and computing devices just as
everyone has familiarity with automotive basics. However, just because you drive a car
everyday doesn’t mean you can tell the diﬀerence between a crankshaft and a piston. To
get started, let’s familiarize ourselves with some basic concepts.
A computer is a device, usually electronic, that stores, receives, processes, and outputs
information. Modern computing devices include everything from simple sensors to mobile
devices, tablets, desktops, mainframes/servers, supercomputers and huge grid clusters
consisting of multiple computers networked together.
Computer hardware usually refers to the physical components in a computing system
which includes input devices such as a mouse/touchpad, keyboard, or touchscreen, output
devices such as monitors, storage devices such as hard disks and solid state drives, as
well as the electronic components such as graphics cards, main memory, motherboards
and chips that make up the Central Processing Unit (CPU).
Computer processors are complex electronic circuits (referred to as Very Large Scale Inte-
gration (VLSI)) which contain thousands of microscopic electronic transistors–electronic
“gates” that can perform logical operations and complex instructions. In addition to
the CPU a processor may contain an Arithmetic and Logic Unit (ALU) that performs
arithmetic operations such as addition, multiplication, division, etc.
Computer Software usually refers to the actual machine instructions that are run on a
processor. Software is usually written in a high-level programming language such as C or
Java and then converted to machine code that the processor can execute.
Computers “speak” in binary code. Binary is nothing more than a structured collection
of 0s and 1s. A single 0 or 1 is referred to as a bit. Bits can be collected to form larger
chunks of information: 8 bits form a byte, 1024 bytes is referred to as a kilobyte, etc.
Table 1.1 contains a several more binary units. Each unit is in terms of a power of 2
instead of a power of 10. As humans, we are more familiar with decimal–base-10 numbers
and so units are usually expressed as powers of 10, kilo- refers to 103, mega- is 106, etc.
However, since binary is base-2 (0 or 1), units are associated with the closest power
of 2. Computers are binary machines because it is the most practical to implement in
electronic devices. 0s and 1s can be easily represented by low/high voltage; low/high
frequency; on-oﬀ; etc. It is much easier to design and implement systems that switch
between only two states.

Table 1.1.: Various units of digital information with respect to bytes. Memory is usually
measured using powers of two.
Computer memory can refer to secondary memory which are typically longterm storage
devices such as hard disks, flash drives, SD cards, optical disks (CDs, DVDs), etc. These
generally have a large capacity but are slower (the time it takes to access a chunk of data
is longer). Or, it can refer to main memory (or primary memory): data stored on chips
that is much faster but also more expensive and thus generally smaller.
The first hard disk (IBM 350) was developed in 1956 by IBM and had a capacity of
3.75MB and cost $3,200 ($27,500 in 2015 dollars) per month to lease. For perspective,
the first commercially available TB hard drive was released in 2007. As of 2015, terabyte
hard disks can be commonly purchased for $50–$100.
Main memory, sometimes referred to as Random Access Memory (RAM) consists of
a collection of addresses along with contents. An address usually refers to a single
byte of memory (called byte-addressing). The content, that is the byte of data that is
stored at an address, can be anything. It can represent a number, a letter, etc. To the
computer it is all just a bunch of 0s and 1s. For convenience, memory addresses are
represented using hexadecimal, which is a base-16 counting system using the symbols
0,1,...,9,a,b,c,d,e,f. Numbers are prefixed with a 0x to indicate they represent
hexadecimal numbers. Figure 1.1 depicts memory and its address/contents.
Separate computing devices can be connected to each other through a network. Networks
can be wired with electrical signals or light as in fiber optics which provide large bandwidth
(the amount of data that can be sent at any one time), but can be expensive to build and
maintain. They can also be wireless, but provide shorter range and lower bandwidth.
1.3. Basic Program Structure
Programs start out as source code, a collection of instructions usually written in a high-
level programming language. A source file containing source code is nothing more than
Introduction
Address Contents
Figure 1.1.: Depiction of Computer Memory. Each address refers to a byte, but diﬀerent
types of data (integers, floating-point numbers, characters) may require
diﬀerent amounts of memory. Memory addresses and some data is represented
in hexadecimal.
6
1.3. Basic Program Structure
a plain text file that can be edited by any text editor. However, many developers and
programmers utilize modern Integrated Development Environment (IDE) that provide a
text editor with code highlighting: various elements are displayed in diﬀerent colors to
make the code more readable and elements can be easily identified. Mistakes such as
unclosed comments or curly brackets can be readily apparent with such editors. IDEs can
also provide automated compile/build features and other tools that make the development
process easier and faster.
Some languages are compiled languages meaning that a source file must be translated
into machine code that a processor can understand and execute. This is actually a
multistep process. A compiler may first preprocess the source file(s) and perform some
pre-compiler operations. It may then transform the source code into another language
such as an assembly language, a lower-level more machine-like language. Ultimately, the
compiler transforms the source code into object code, a binary format that the machine
can understand.
To produce an executable file that can actually be run, a linker may then take the object
code and link in any other necessary objects or precompiled library code necessary to
produce a final program. Finally, an executable file (still just a bunch of binary code) is
produced.
Once an executable file has been produced we can run the program. When a program is
executed, a request is sent to the operating system to load and run the program. The
operating system loads the executable file into memory and may setup additional memory
for its variables as well as its call stack (memory to enable the program to make function
calls). Once loaded and setup, the operating system begins executing the instructions at
the program’s entry point.
In many languages, a program’s entry point is defined by a main function or method.
A program may contain many functions and pieces of code, but this special function is
defined as the one that gets invoked when a program starts. Without a main function,
the code may still be useful: libraries contain many useful functions and procedures so
that you don’t have to write a program from scratch. However, these functions are not
intended to be run by themselves. Instead, they are written so that other programs can
use them. A program becomes executable only when a main entry point is provided.
This compile-link-execute process is roughly depicted in Code Sample 1.2. An example of
a simple C program can be found in Code Sample 1.1 along with the resulting assembly
code produced by a compiler in Figure 1.2 and the final machine code represented in
hexadecimal in Code Sample 1.3.
In contrast, some languages are interpreted, not compiled. The source code is contained
in a file usually referred to as a script. Rather than being run directly by an operating
system, the operating system loads and execute another program called an interpreter.
The interpreter then loads the script, parses, and execute its instructions. Interpreted
7
1. Introduction
Text Editor
or IDE Syntax
Error(s)
8
Source File
Compiler
Other Object
Files &
Libraries
success
Object File Linker
Executable
File
Figure 1.2.: A Compiling Process
Input
run
Results &
Output
1.3. Basic Program Structure
1 #include<stdlib.h>
2 #include<stdio.h>
3 #include<math.h>
4
5 int main(int argc, char **argv) {
6
7 if(argc != 2) {
8 fprintf(stderr, "Usage: %s x\n", argv[0]);
9 exit(1);
10 }
11
12 double x = atof(argv[1]);
13 double result = sqrt(x);
14
15 if(x < 0) {
16 17 exit(2);
18 }
fprintf(stderr, "Cannot handle complex roots\n");
19
20 21
22 return 0;
23 }
printf("square root of %f = %f\n", x, result);
Code Sample 1.1.: A simple program in C
Introduction
languages may still have a predefined main function, but in general, a script starts
executing starting with the first instruction in the script file. Adhering to the syntax
rules is still important, but since interpreted languages are not compiled, syntax errors
become runtime errors. A program may run fine until its first syntax error at which
point it fails.
There are other ways of compiling and running programs. Java for example represents a
compromise between compiled and interpreted languages. Java source code is compiled
into Java bytecode which is not actually machine code that the operating system and
hardware can run directly. Instead, it is compiled code for a Java Virtual Machine (JVM).
This allows a developer to write highly portable code, compile it once and it is runnable
on any JVM on any system (write-once, compile-once, run-anywhere).
In general, interpreted languages are slower than compiled languages because they are
being run through another program (the interpreter) instead of being executed directly
by the processor. Modern tools have been introduced to solve this problem. Just In Time
(JIT) compilers have been developed that take scripts that are not usually compiled,
and compile them to a native machine code format which has the potential to run much
faster than when interpreted. Modern web browsers typically do this for JavaScript code
(Google Chrome’s V8 JavaScript engine for example).
Another related technology are transpilers. Transpilers are source-to-source compilers.
They don’t produce assembly or machine code, instead they translate code in one
high-level programming language to another high-level programming language. This
is sometimes done to ensure that scripting languages like JavaScript are backwards
compatible with previous versions of the language. Transpilers can also be used to
translate one language into the same language but with diﬀerent aspects (such as parallel
or synchronized code) automatically added. They can also be used to translate older
languages such as Pascal to more modern languages as a first step in updating a legacy
system.
1.4. Syntax Rules & Pseudocode
Programming languages are a lot like human languages in that they have syntax rules.
These rules dictate the appropriate arrangements of words, punctuation, and other
symbols that form valid statements in the language. For example, in many programming
languages, commands or statements are terminated by semicolons (just as most sentences
are ended with a period). This is an example of “punctuation” in a programming language.
In English paragraphs are separated by lines, in programming languages blocks of code
are separated by curly brackets. Variables are comparable to nouns and operations and
functions are comparable to verbs. Complex documents often have footnotes that provide
additional explanations; code has comments that provide documentation and explanation
for important elements. English is read top-to-bottom, left-to-right. Programming
12
1.4. Syntax Rules & Pseudocode
languages are similar: individual executable commands are written one per line. When a
program executes, each command executes one after the other, top-to-bottom. This is
known as sequential control flow.
A block of code is a section of code that has been logically grouped together. Many
languages allow you to define a block by enclosing the grouped code around opening and
closing curly brackets. Blocks can be nested within each other to form sub-blocks.
Most languages also have reserved words and symbols that have special meaning. For
example, many languages assign special meaning to keywords such as for, if, while,
etc. that are used to define various control structures such as conditionals and loops.
Special symbols include operators such as + and * for performing basic arithmetic.
Failure to adhere to the syntax rules of a particular language will lead to bugs and
programs that fail to compile and/or run. Natural languages such as English are very
forgiving: we can generally discern what someone is trying to say even if they speak
in broken English (to a point). However, a compiler or interpreter isn’t as smart as a
human. Even a small syntax error (an error in the source code that does not conform
to the language’s rules) will cause a compiler to completely fail to understand the code
you have written. Learning a programming language is a lot like learning a new spoken
language (but, fortunately a lot easier).
In subsequent parts of this book we focus on particular languages. However, in order
to focus on concepts, we’ll avoid specific syntax rules by using pseudocode, informal,
high-level descriptions of algorithms and processes. Good pseudocode makes use of plain
English and mathematical notation, making it more readable and abstract. A small
example can be found in Algorithm 1.1.
Input : A collection of numbers, A= {a1,a2,...,an}
Output : The minimal element in A
1 Let min be equal to a1
2 foreach element ai in A do
3 if ai <min then
4 ai is less than the smallest element we’ve found so far
5 Update min to be equal to ai
6 end
7 end
8 output min
Algorithm 1.1: An example of pseudocode: finding a minimum value
13
1. Introduction
1.5. Documentation, Comments, and Coding Style
Good code is not just functional, it is also beautiful. Good code is organized, easy to
read, and well documented. Organization can be achieved by separating code into useful
functions and collecting functions into modules or libraries. Good organization means
that at any one time, we only need to focus on a small part of a program.
It would be diﬃcult to read an essay that contained random line breaks, paragraphs
were not indented, it contained diﬀerent spacing or diﬀerent fonts, etc. Likewise, code
should be legible. Well written code is consistent and makes good use of whitespace
and indentation. Code within the same code block should be indented at the same level.
Nested blocks should be further indented just like the outline of an essay or table of
contents.
Code should also be well documented. Each line or segment of code should be clear
enough that it tells the user what the code does and how it does it. This is referred to as
“self-documenting” code. A person familiar with the particular language should be able
to read your code and immediately understand what it does. In addition, well-written
code should contain suﬃcient and clear comments. A comment in a program is intended
for a human user to read. A comment is ultimately ignored by the compiler/interpreter
and has no eﬀect on the actual program. Good comments tell the user why the code was
written or why it was written the way it was. Comments provide a high-level description
of what a block of code, function, or program does. If the particular method or algorithm
is of interest, it should also be documented.
There are typically two ways to write comments. Single line comments usually begin
with two forward slashes, //.
1 Everything after the slashes until the next line is ignored.
Multiline comments begin with a /* and end with a */; everything between them is
ignored even if it spans multiple lines. This syntax is shared among many languages
including C, Java, PHP and others. Some examples:
1 double x = sqrt(y); //this is a single line comment
2
3 /*
4 This is a multiline comment
5 each line is ignored, but allows
6 for better formatting
7 */
8
10 9 /**
* This is a doc-style comment, usually placed in
/ and a backslash 1You can remember the diﬀerence between a forward slash person facing right and either leaning backwards (backslash) or forwards (forward slash).
\ by thinking of a
14
1.5. Documentation, Comments, and Coding Style
11 13 * front of major portions of code such as a function
12 * to provide documentation
* It begins with a forward-slash-star-star
14 */
The last example above is a doc-style comment. It originated with Java, but has since
been adopted by many other programming languages. Syntactically it is a normal
multiline comment, but begins with a /**. Asterisks are aligned together on each
line. Certain commenting systems allow you to place other marked up data inside these
comments such as labeling parameters ( @param x) or use HTML code to provide links
and style. These doc-style comments are used to provide documentation for major parts
of the code especially functions and data structures. Though not part of the language,
other documentation tools can be used to gather the information in doc-style comments
to produce formatted documentation such as web pages or Portable Document Format
(PDF) documents.
Comments should not be trivial: they should not explain something that should be
readily apparent to an experienced user or programmer. For example, if a piece of code
adds two numbers together and stores the result, there should not be a comment that
explains the process. It is a simple and common enough operation that is self-evident.
However, if a function uses a particular process or algorithm such as a Fourier Transform
to perform an operation, it would be appropriate to document it in a series of comments.
Comments can also detail how a function or piece of code should be used. This is
typically done when developing an Application Programmer Interface (API) for use
by other programmers. The API’s available functions should be well documented so
that users will know how and when to use a particular function. It can document the
function’s expectations and behavior such as how it handles bad input or error situations.
15
2. Basics
2.1. Control Flow
The flow of control (or simply control flow) is how a program processes its instructions.
Typically, programs operate in a linear or sequential flow of control. Executable statements
or instructions in a program are performed one after another. In source code, the order
that instructions are written defines their order. Just like English, a program is “read”
top to bottom. Each statement may modify the state of a program. The state of a
program is the value of all its variables and other information/data stored in memory
at a given moment during its execution. Further, an executable statement may instead
invoke (or call or execute) another procedure (also called subroutine, function, method,
etc.) which is another unit of code that has been encapsulated into one unit so that it
can be reused.
This type of control flow is usually associated with a procedural programming paradigm
(which is closely related to imperative or structured programming paradigms). Though
this text will mostly focus on languages that are procedural (or that have strong procedural
aspects), it is important to understand that there are other programming language
paradigms. Functional programming languages such as Scheme and Haskell achieve
computation through the evaluation of mathematical functions with as little or no (“pure”
functional) state at all. Declarative languages such as those used in database languages
like SQL or in spreadsheets like Excel specify computation by expressing the logic of
computation rather than explicitly specifying control flow. For a more formal introduction
to programming language paradigms, a good resource is Seven Languages in Seven Weeks:
A Pragmatic Guide to Learning Programming Languages by Tate [36].
2.1.1. Flowcharts
Sometimes processes are described using diagrams called flowcharts. A flowchart is a
visual representation of an algorithm or process consisting of boxes or “nodes” connected
by directed edges. Boxes can represent an individual step or a decision to be made. The
edges establish an order of operations in the diagram.
Some boxes represent decisions to be made which may have one or more alternate routes
(more than one directed edge going out of the box) depending on the the result of the
17